{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel '.venv (Python 3.12.9)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "pip install transformers datasets tokenizers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pip install numpy==1.26.4 pandas scipy transformers torch datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def load_dataset(csv_path, required_columns=None):\n",
    "    \"\"\"\n",
    "    Loads a dataset from a CSV file, validates required columns, and handles missing values.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Path to the CSV file.\n",
    "        required_columns (list, optional): List of required columns to validate.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Processed pandas DataFrame.\n",
    "    \"\"\"\n",
    "    # üîπ Check if the file exists\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"Error: File not found at {csv_path}\")\n",
    "\n",
    "    # üîπ Load CSV with all columns as strings\n",
    "    df = pd.read_csv(csv_path, dtype=str, sep=\"\\t\")\n",
    "    df.columns = df.columns.str.strip().str.lower()  # Normalize column names\n",
    "    print(\"üîπ Available columns in CSV:\", df.columns.tolist())  # Debugging\n",
    "\n",
    "    # üîπ Validate required columns (if provided)\n",
    "    if required_columns:\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            raise KeyError(f\"Missing columns in CSV: {missing_columns}\")\n",
    "\n",
    "    # üîπ Handle missing values (replace NaNs with empty strings)\n",
    "    df.fillna(\"\", inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# ‚úÖ Example Usage:\n",
    "csv_path = \"D:\\\\train\\\\csv_output\\\\med.csv\"\n",
    "required_columns = [\"condition\", \"symptoms\", \"duration\", \"severity\", \"risk_factors\", \"suggested_action\"]\n",
    "\n",
    "try:\n",
    "    dataset = load_dataset(csv_path, required_columns)\n",
    "    print(\"‚úÖ Dataset loaded successfully!\")\n",
    "    print(dataset.head())  # Show first few rows\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ‚úÖ Load CSV\n",
    "csv_path = \"D:\\\\trail\\\\csv_output\\\\med.csv\"\n",
    "df = pd.read_csv(csv_path, delimiter=\"\\t\")  # Adjust delimiter if needed\n",
    "\n",
    "# ‚úÖ Define required columns\n",
    "required_columns = [\"condition\", \"symptoms\", \"duration\", \"severity\", \"risk_factors\", \"suggested_action\"]\n",
    "\n",
    "# ‚úÖ Check if required columns are present\n",
    "if not set(required_columns).issubset(df.columns):\n",
    "    raise ValueError(f\"Missing required columns: {set(required_columns) - set(df.columns)}\")\n",
    "\n",
    "# ‚úÖ Print total number of rows\n",
    "print(f\"Total rows in dataset: {len(df)}\")\n",
    "\n",
    "# ‚úÖ Sample up to 25 rows (but don't fail if fewer rows exist)\n",
    "df_test = df.sample(n=min(11, len(df)), random_state=42)\n",
    "\n",
    "# ‚úÖ Save only required columns\n",
    "df_test[required_columns].to_csv(\"eval_texts.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"‚úÖ eval_texts.csv saved successfully with required columns!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_path, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = \"D:\\\\trail\\\\custom_model\\\\tokenizer.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "tokenizer_path = \"D:\\\\trail\\\\custom_model\\\\tokenizer_config.json\"\n",
    "\n",
    "try:\n",
    "    with open(tokenizer_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        json.load(file)  # Try loading JSON\n",
    "    print(\"‚úÖ JSON is valid!\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"‚ùå JSON format error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from datasets import Dataset\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "# ‚úÖ Step 1: Load the Custom Tokenizer\n",
    "tokenizer_path = \"D:\\\\trail\\\\custom_model\\\\tokenizer.json\"\n",
    "if not os.path.exists(tokenizer_path):\n",
    "    raise FileNotFoundError(f\"Tokenizer file not found: {tokenizer_path}\")\n",
    "\n",
    "custom_tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "\n",
    "# ‚úÖ Step 2: Load Evaluation Texts from CSV\n",
    "eval_csv_path = \"D:\\\\trail\\\\csv_output\\\\eval_texts.csv\"\n",
    "if not os.path.exists(eval_csv_path):\n",
    "    raise FileNotFoundError(f\"CSV file not found: {eval_csv_path}\")\n",
    "\n",
    "df_eval = pd.read_csv(eval_csv_path)\n",
    "\n",
    "# ‚úÖ Ensure required columns exist\n",
    "required_columns = [\"condition\", \"symptoms\", \"duration\", \"severity\", \"risk_factors\", \"suggested_action\"]\n",
    "missing_columns = [col for col in required_columns if col not in df_eval.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "\n",
    "# ‚úÖ Combine columns into a single text input\n",
    "def combine_text(row):\n",
    "    return f\"Condition: {row['condition']} | Symptoms: {row['symptoms']} | Duration: {row['duration']} | \" \\\n",
    "           f\"Severity: {row['severity']} | Risk Factors: {row['risk_factors']} | Suggested Action: {row['suggested_action']}\"\n",
    "\n",
    "eval_texts = df_eval.apply(combine_text, axis=1).tolist()\n",
    "eval_labels = list(range(len(eval_texts)))  # Sequential labels for evaluation\n",
    "\n",
    "# ‚úÖ Step 3: Tokenize Evaluation Texts\n",
    "MAX_LEN = 128\n",
    "\n",
    "def preprocess_text(text):\n",
    "    encoded = custom_tokenizer.encode(text)\n",
    "    input_ids = encoded.ids[:MAX_LEN]  # Truncate if longer\n",
    "    input_ids += [0] * (MAX_LEN - len(input_ids))  # Pad if shorter\n",
    "    return input_ids\n",
    "\n",
    "tokenized_eval = [preprocess_text(text) for text in eval_texts]\n",
    "\n",
    "# ‚úÖ Step 4: Convert Tokenized Data into a Dataset\n",
    "tokenized_eval_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": tokenized_eval,\n",
    "    \"labels\": eval_labels\n",
    "})\n",
    "\n",
    "# ‚úÖ Step 5: Define and Load the Trained Model\n",
    "class SimpleLSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, num_labels):\n",
    "        super(SimpleLSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, 128)\n",
    "        self.lstm = nn.LSTM(128, 256, batch_first=True)\n",
    "        self.fc = nn.Linear(256, num_labels)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embedding(input_ids)\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        logits = self.fc(hidden[-1])\n",
    "        return logits\n",
    "\n",
    "# ‚úÖ Step 6: Load Model Weights\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# üõ† Determine `vocab_size`\n",
    "try:\n",
    "    vocab_size = len(custom_tokenizer.get_vocab())  # Preferred way\n",
    "except AttributeError:\n",
    "    vocab_size = 30522  # Default to BERT vocab size if `get_vocab()` fails\n",
    "\n",
    "# üõ† Load metadata (if saved) to get correct num_labels\n",
    "metadata_path = \"D:\\\\trail\\\\custom_model.pth\"\n",
    "if os.path.exists(metadata_path):\n",
    "    metadata = torch.load(metadata_path)\n",
    "    num_labels = metadata.get(\"num_labels\", 8)  # Default to 8 if missing\n",
    "else:\n",
    "    num_labels = 8  # Set manually if metadata is missing\n",
    "\n",
    "# ‚úÖ Initialize Model\n",
    "model = SimpleLSTMModel(vocab_size, num_labels).to(device)\n",
    "model_weights_path = \"D:\\\\trail\\\\custom_model.pth\"\n",
    "\n",
    "if not os.path.exists(model_weights_path):\n",
    "    raise FileNotFoundError(f\"Model weights file not found: {model_weights_path}\")\n",
    "\n",
    "# üõ† Load Model Weights Safely\n",
    "model.load_state_dict(torch.load(model_weights_path, map_location=device), strict=False)\n",
    "model.eval()\n",
    "\n",
    "# ‚úÖ Step 7: Get Predictions on Evaluation Set\n",
    "input_tensor = torch.tensor(tokenized_eval, dtype=torch.long).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_tensor)\n",
    "\n",
    "predicted_labels = torch.argmax(logits, axis=1).cpu().numpy()\n",
    "\n",
    "# ‚úÖ Step 8: Compute Confusion Matrix\n",
    "true_labels = np.array(eval_labels)\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# ‚úÖ Step 9: Plot Confusion Matrix\n",
    "plt.figure(figsize=(4, 2))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=range(num_labels), yticklabels=range(num_labels))\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# ‚úÖ Step 10: Print Classification Report\n",
    "print(classification_report(true_labels, predicted_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "class CustomBertForSequenceClassification(BertForSequenceClassification):\n",
    "    def __init__(self, config, class_weights):\n",
    "        super().__init__(config)\n",
    "        self.class_weights = class_weights.to(config.device)  # Move to GPU if available\n",
    "        self.loss_fn = nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        outputs = super().forward(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "            return {\"loss\": loss, \"logits\": logits}\n",
    "        \n",
    "        return {\"logits\": logits}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"eval_texts.csv\")  # Load again with correct column names\n",
    "print(df.columns)  # Check if the original names are back\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore correct column names based on the actual dataset\n",
    "df.columns = [\"condition\", \"symptoms\", \"duration\", \"severity\", \"risk_factors\", \"suggested_action\"]\n",
    "\n",
    "# Verify columns are correctly restored\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Ensure the 'condition' column (acting as labels) is correct\n",
    "if \"condition\" not in df.columns:\n",
    "    raise KeyError(\"üö® Column 'condition' not found! Check column names:\", df.columns)\n",
    "\n",
    "# Ensure 'condition' values are valid\n",
    "df = df.dropna(subset=[\"condition\"])  # Remove NaNs\n",
    "df[\"condition\"] = df[\"condition\"].astype(\"category\").cat.codes  # Convert conditions to numerical labels\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(df[\"condition\"]),\n",
    "    y=df[\"condition\"]\n",
    ")\n",
    "\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "print(\"‚úÖ Fixed & computed class weights:\", class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Define correct labels column based on dataset\n",
    "label_column = \"condition\"  # Adjust if needed\n",
    "\n",
    "# Compute class weights\n",
    "unique_classes = np.unique(df[label_column])\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=unique_classes,\n",
    "    y=df[label_column]\n",
    ")\n",
    "\n",
    "# Convert to tensor\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "print(\"Fixed & computed class weights:\", class_weights)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    eval_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "# Define loss function with class weights\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Custom Trainer class to override compute_loss\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):  # ‚úÖ FIXED: Accept additional args\n",
    "        labels = inputs.pop(\"labels\")  # Extract labels\n",
    "        outputs = model(**inputs)  # Forward pass\n",
    "        logits = outputs.logits  # Extract logits\n",
    "\n",
    "        # Ensure correct shape for loss function\n",
    "        if logits.shape[-1] == 1:  \n",
    "            logits = logits.squeeze(-1)\n",
    "\n",
    "        loss = loss_fn(logits, labels)  # Compute weighted loss\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Initialize trainer with custom loss function\n",
    "trainer = WeightedLossTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Training and validation loss values\n",
    "epochs = list(range(1, 11))  # Number of epochs (1 to 10)\n",
    "training_loss = [1.9648204803466798] * 10  # Repeat last reported training loss\n",
    "validation_loss = [2.465202, 2.690884, 2.940101, 2.937016, 3.000245, 3.014100, 3.037950, 3.019710, 3.039859, 3.041418]\n",
    "\n",
    "# Plot graph\n",
    "plt.figure(figsize=(4, 2))\n",
    "plt.plot(epochs, training_loss, label=\"Training Loss\", marker='o', linestyle='--', color='blue')\n",
    "plt.plot(epochs, validation_loss, label=\"Validation Loss\", marker='s', linestyle='-', color='red')\n",
    "\n",
    "# Labels & title\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training vs. Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={\"condition\": \"label\"}, inplace=True)\n",
    "df.rename(columns={\"symptoms\": \"label\"}, inplace=True)\n",
    "df.rename(columns={\"duration\": \"label\"}, inplace=True)\n",
    "df.rename(columns={\"severity\": \"label\"}, inplace=True)\n",
    "df.rename(columns={\"risk_factors\": \"label\"}, inplace=True)\n",
    "df.rename(columns={\"suggested_action\": \"label\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# ‚úÖ Save the trained model using torch.save()\n",
    "torch.save(model.state_dict(), \"custom_model.pth\")\n",
    "\n",
    "# ‚úÖ Save the tokenizer (if using a custom tokenizer)\n",
    "custom_tokenizer.save(\"saved_tokenizer.json\")\n",
    "\n",
    "print(\"Model and tokenizer saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, output_dim=2):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        _, (hidden, _) = self.lstm(embedded)\n",
    "        logits = self.fc(hidden[-1])  # Extract final hidden state\n",
    "        return {\"logits\": logits}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# ‚úÖ Fine-tuning with adjusted hyperparameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tuned_model\",\n",
    "    per_device_train_batch_size=32,  # Increase batch size if you have enough memory\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=2e-5,  # Lower learning rate for better fine-tuning\n",
    "    num_train_epochs=5,  # Train for more epochs\n",
    "    weight_decay=0.01,  # Helps prevent overfitting\n",
    "    eval_strategy=\"epoch\",  # Evaluate after every epoch\n",
    "    save_strategy=\"epoch\",  # Save model checkpoints\n",
    "    logging_dir=\"./logs\",  # Log training data\n",
    "    load_best_model_at_end=True,  # Use best model based on evaluation loss\n",
    ")\n",
    "\n",
    "print(\"Hyperparameters updated!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# ‚úÖ Fine-tuning with adjusted hyperparameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tuned_model\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",  # ‚úÖ Updated from evaluation_strategy\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "print(\"Hyperparameters updated!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "model_dir = \"custom_model\"\n",
    "if os.path.exists(model_dir):\n",
    "    print(\"Directory exists:\", os.listdir(model_dir))\n",
    "else:\n",
    "    print(\"Model directory not found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV into Pandas DataFrame\n",
    "df = pd.read_csv(\"D:\\\\trail\\\\csv_output\\\\med.csv\")\n",
    "\n",
    "# Check if df is loaded correctly\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Convert Pandas DataFrame to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Split into train/test (80-20 split)\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Now you should have `dataset[\"train\"]` and `dataset[\"test\"]`\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['condition', 'symptoms', 'duration', 'severity', 'risk_factors', 'suggested_action', 'condition_id']\n",
    ",  # Replace \"text\" with the correct column name\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    if \"Symptom,Duration,Severity,Possible Conditions,Risk Factors,Suggested Action\" not in examples:  # Adjust to actual column name\n",
    "        raise KeyError(f\"Expected column 'text' not found. Available columns: {examples.keys()}\")\n",
    "    return tokenizer(\n",
    "        examples[\"condition,symptoms,duration,severity,risk_factors,suggested_action\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "model_path = \"D:\\\\trail\\\\custom_model\"\n",
    "print(os.listdir(model_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "save_path = \"D:\\\\trail\\\\custom_model\"\n",
    "torch.save(model.state_dict(), save_path + \"/pytorch_model.bin\")\n",
    "print(f\"Custom model saved to: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"D:\\\\trail\\\\custom_model/pytorch_model.bin\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_symptoms(user_input):\n",
    "    # ‚úÖ Tokenize input text\n",
    "    inputs = tokenizer(user_input, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "\n",
    "    # ‚úÖ Move inputs to the same device as the model\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "    # ‚úÖ Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # ‚úÖ Get the predicted label\n",
    "    predicted_class = torch.argmax(outputs.logits, dim=1).item()\n",
    "\n",
    "    return predicted_class  # Return predicted label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ‚úÖ Load tokenizer from a working model (e.g., 'bert-base-uncased' or your trained tokenizer)\n",
    "model_checkpoint = \"bert-base-uncased\"  # Change this if you trained a custom tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# ‚úÖ Re-save tokenizer with all required files\n",
    "save_path = \"D:\\\\trail\\\\custom_tokenizer\"\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\"‚úÖ Tokenizer re-saved to: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"D:\\\\trail\\\\custom_tokenizer\")\n",
    "print(\"‚úÖ Tokenizer loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_path, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns=lambda x: x.strip(), inplace=True)  # Remove leading/trailing spaces\n",
    "df.rename(columns={'symptoms': 'Symptoms', 'condition': 'Condition'}, inplace=True)  # Adjust based on actual names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_path, delimiter=\",\")  # Change delimiter if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.path.exists(csv_path))  # Should print True if the file exists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Symptoms' not in df.columns:\n",
    "    print(\"Symptoms column is missing! Available columns:\", df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install speechrecognition gtts pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install playsound==1.2.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "from gtts import gTTS\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import threading\n",
    "from playsound import playsound\n",
    "\n",
    "# ‚úÖ Load dataset\n",
    "csv_path = \"D:\\\\trail\\\\csv_output\\\\eval_texts.csv\"\n",
    "\n",
    "# Read CSV and handle delimiter issues\n",
    "df = pd.read_csv(csv_path, delimiter=\",\")  # Try \"\\t\" if comma doesn't work\n",
    "\n",
    "# Fix merged column issue\n",
    "if len(df.columns) == 1:\n",
    "    df = df[df.columns[0]].str.split(\",\", expand=True)  # Split into columns\n",
    "    df.columns = df.iloc[0]  # Set first row as column names\n",
    "    df = df[1:].reset_index(drop=True)  # Remove the first row\n",
    "\n",
    "# Clean column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Required columns check\n",
    "required_columns = [\"condition\", \"symptoms\", \"duration\", \"severity\", \"risk_factors\", \"suggested_action\"]\n",
    "missing_columns = set(required_columns) - set(df.columns)\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"‚ùå Missing required columns: {missing_columns}. Available columns: {df.columns}\")\n",
    "\n",
    "# Convert symptoms column to lowercase for better matching\n",
    "df[\"symptoms\"] = df[\"symptoms\"].astype(str).str.lower()\n",
    "\n",
    "# ‚úÖ Speak function using gTTS\n",
    "def speak(text):\n",
    "    \"\"\"Convert text to speech using gTTS and play it.\"\"\"\n",
    "    print(f\"\\nMira üë©‚Äç‚öïÔ∏è: {text}\")\n",
    "    tts = gTTS(text=text, lang='en')\n",
    "    temp_audio = \"temp_audio.mp3\"\n",
    "    tts.save(temp_audio)\n",
    "    playsound(temp_audio)\n",
    "    os.remove(temp_audio)\n",
    "\n",
    "# ‚úÖ Initialize speech recognition\n",
    "recognizer = sr.Recognizer()\n",
    "mic = sr.Microphone() if sr.Microphone.list_microphone_names() else None\n",
    "\n",
    "# ‚úÖ Listen function (Continuous mode)\n",
    "def listen():\n",
    "    \"\"\"Continuously listens for user input.\"\"\"\n",
    "    if not mic:\n",
    "        speak(\"No microphone detected. Please check your device settings.\")\n",
    "        return \"\"\n",
    "\n",
    "    with mic as source:\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "        print(\"\\nüé§ Listening... (Say 'exit' to quit)\")\n",
    "        audio = recognizer.listen(source)\n",
    "\n",
    "    try:\n",
    "        user_input = recognizer.recognize_google(audio).lower()\n",
    "        print(f\"User üé§: {user_input}\")\n",
    "        return user_input\n",
    "    except sr.UnknownValueError:\n",
    "        speak(\"Sorry, I didn‚Äôt catch that. Could you repeat?\")\n",
    "        return \"\"\n",
    "    except sr.RequestError:\n",
    "        speak(\"I'm having trouble understanding right now. Please try again.\")\n",
    "        return \"\"\n",
    "\n",
    "# ‚úÖ Extract numeric duration from user input\n",
    "def extract_duration(text):\n",
    "    \"\"\"Extracts the first number found in the text.\"\"\"\n",
    "    match = re.search(r\"\\d+\", text)\n",
    "    return int(match.group()) if match else None\n",
    "\n",
    "# ‚úÖ Check if duration is valid\n",
    "def is_duration_valid(user_duration, duration_range):\n",
    "    \"\"\"Checks if the user's duration falls within the range specified in the dataset.\"\"\"\n",
    "    try:\n",
    "        user_duration = int(user_duration)\n",
    "        match = re.match(r\"(\\d+)-(\\d+)\", duration_range)\n",
    "        if match:\n",
    "            lower, upper = map(int, match.groups())\n",
    "            return lower <= user_duration <= upper\n",
    "        return user_duration == int(duration_range)\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "# ‚úÖ Live Consultation\n",
    "def start_live_consultation():\n",
    "    \"\"\"Keeps Mira AI running for live interaction.\"\"\"\n",
    "    speak(\"Hello, I'm Dr. Mira. You can start by saying your symptoms.\")\n",
    "    \n",
    "    while True:\n",
    "        # Step 1: Ask for symptoms\n",
    "        speak(\"What symptoms are you experiencing?\")\n",
    "        symptom = listen().strip()\n",
    "        if symptom == \"exit\":\n",
    "            speak(\"Goodbye! Stay healthy.\")\n",
    "            break\n",
    "\n",
    "        matched_conditions = df[df[\"symptoms\"].str.contains(symptom, na=False, case=False)]\n",
    "        if matched_conditions.empty:\n",
    "            speak(\"I couldn't find any condition with that symptom. Please try again.\")\n",
    "            continue\n",
    "\n",
    "        speak(f\"I found {len(matched_conditions)} conditions related to {symptom}. Let's narrow it down.\")\n",
    "\n",
    "        # Step 2: Ask for duration\n",
    "        speak(\"For how many days have you been experiencing this?\")\n",
    "        duration_input = listen().strip()\n",
    "        if duration_input == \"exit\":\n",
    "            speak(\"Goodbye! Stay healthy.\")\n",
    "            break\n",
    "\n",
    "        duration_number = extract_duration(duration_input)\n",
    "        if not duration_number:\n",
    "            speak(\"I didn't hear a valid duration. Please try again.\")\n",
    "            continue\n",
    "\n",
    "        # ‚úÖ Filter conditions based on duration range\n",
    "        valid_conditions = matched_conditions[\n",
    "            matched_conditions[\"duration\"].apply(lambda x: is_duration_valid(duration_number, str(x)))\n",
    "        ]\n",
    "\n",
    "        if valid_conditions.empty:\n",
    "            speak(\"No conditions match that duration range. Please consult a doctor.\")\n",
    "            continue\n",
    "\n",
    "        speak(f\"{len(valid_conditions)} conditions match your symptom and duration.\")\n",
    "\n",
    "        # Step 3: Ask for severity\n",
    "        speak(\"Is it mild, moderate, or severe?\")\n",
    "        severity = listen().strip()\n",
    "        if severity == \"exit\":\n",
    "            speak(\"Goodbye! Stay healthy.\")\n",
    "            break\n",
    "\n",
    "        valid_conditions = valid_conditions[valid_conditions[\"severity\"].str.contains(severity, na=False, case=False)]\n",
    "        if valid_conditions.empty:\n",
    "            speak(\"No conditions match that severity level. Please consult a doctor.\")\n",
    "            continue\n",
    "\n",
    "        speak(f\"{len(valid_conditions)} conditions match your symptom, duration, and severity.\")\n",
    "\n",
    "        # Step 4: Ask about risk factors\n",
    "        risk_factors = valid_conditions[\"risk_factors\"].dropna().unique()\n",
    "        risk_factors_text = \", \".join(risk_factors)\n",
    "\n",
    "        if risk_factors_text:\n",
    "            speak(f\"Based on the conditions found, some possible risk factors are: {risk_factors_text}. Do any of these apply to you?\")\n",
    "            user_risk_factor = listen().strip()\n",
    "            if user_risk_factor == \"exit\":\n",
    "                speak(\"Goodbye! Stay healthy.\")\n",
    "                break\n",
    "\n",
    "            if user_risk_factor:\n",
    "                valid_conditions = valid_conditions[valid_conditions[\"risk_factors\"].str.contains(user_risk_factor, na=False, case=False)]\n",
    "        \n",
    "        # Step 5: Identify the most probable condition\n",
    "        if valid_conditions.empty:\n",
    "            speak(\"I couldn't find a perfect match. However, based on the symptoms and severity, you should consult a doctor.\")\n",
    "            continue\n",
    "\n",
    "        condition_name = valid_conditions.iloc[0][\"condition\"]\n",
    "        suggested_action = valid_conditions.iloc[0][\"suggested_action\"]\n",
    "\n",
    "        speak(f\"It looks like you might have {condition_name}. {suggested_action}\")\n",
    "\n",
    "# ‚úÖ Start live chatbot\n",
    "start_live_consultation()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
